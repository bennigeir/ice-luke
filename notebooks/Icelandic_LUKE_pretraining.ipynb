{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Icelandic-LUKE-pretraining.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "t1VgSnwGmCuY",
        "ZAlEeGHKhCJj",
        "Np0rFzQ3hEtx",
        "zyKHysiTgsBM",
        "_bbJIKTNYpiC",
        "gkxk1FPZux1l"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-training of Icelandic version of LUKE**\n",
        "\n",
        "Heavily based on code available at https://github.com/studio-ousia/luke\n",
        "\n",
        "\n",
        "These instructions should work for most languages listed [here](https://en.wikipedia.org/wiki/List_of_Wikipedias)."
      ],
      "metadata": {
        "id": "WADwetfYlzPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **Creation of Icelandic pretraining dataset**\n",
        "\n",
        "1. Download relevant wikipedia dump [here](https://dumps.wikimedia.org). \n",
        "The dump file used in this experiment can be found [here](https://dumps.wikimedia.org/iswiki/20220101/).\n",
        "2. Process the wikipedia dump file.\n",
        "3. Changes made to original code for encoding. *Explain and reference github where code is stored.*\n",
        "4. Create pretraining dataset."
      ],
      "metadata": {
        "id": "t1VgSnwGmCuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Build DB dump file\n",
        "\n",
        "```\n",
        "Usage: !python -m luke.cli build-dump-db [OPTIONS] DUMP_FILE OUT_FILE\n",
        "\n",
        "Options:\n",
        "  --pool-size INTEGER\n",
        "  --chunk-size INTEGER\n",
        "  --help                Show this message and exit.\n",
        "```"
      ],
      "metadata": {
        "id": "ZAlEeGHKhCJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m luke.cli build-dump-db \\\n",
        "iswiki-20220101-pages-articles.xml.bz2 \\\n",
        "iswiki-20220101"
      ],
      "metadata": {
        "id": "R9C_LiINe5my",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8005cc15-00dd-4b2c-8405-cb3cfbf7a56f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: Error while finding module specification for 'luke.cli' (ModuleNotFoundError: No module named 'luke')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Build entity vocabulary file\n",
        "\n",
        "```\n",
        "Usage: !python -m luke.cli build-entity-vocab [OPTIONS] DUMP_DB_FILE OUT_FILE\n",
        "\n",
        "Options:\n",
        "  --vocab-size INTEGER\n",
        "  -w, --white-list FILENAME\n",
        "  --white-list-only\n",
        "  --pool-size INTEGER\n",
        "  --chunk-size INTEGER\n",
        "  --help                     Show this message and exit.\n",
        "  ```"
      ],
      "metadata": {
        "id": "Np0rFzQ3hEtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m luke.cli build-entity-vocab \\\n",
        "iswiki-20220101 \\\n",
        "is-entity-vocab"
      ],
      "metadata": {
        "id": "zVVK-KjBhHpw",
        "outputId": "4c11abe5-9e8a-4ad8-f498-92e03346a78a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: Error while finding module specification for 'luke.cli' (ModuleNotFoundError: No module named 'luke')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Build pre-training dataset\n",
        "\n",
        "```Usage: python -m luke.cli build-wikipedia-pretraining-dataset [OPTIONS] DUMP_DB_FILE TOKENIZER_NAME ENTITY_VOCAB_FILE OUTPUT_DIR```"
      ],
      "metadata": {
        "id": "zyKHysiTgsBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m luke.cli build-wikipedia-pretraining-dataset \\\n",
        "iswiki-20220101 \\\n",
        "mideind/IceBERT-igc \\\n",
        "is-entity-vocab \\\n",
        "./is-wikipedia-pretrain-dataset/"
      ],
      "metadata": {
        "id": "veUeldDcwCKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395f76cd-3187-4633-cfc7-e1046f935763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: Error while finding module specification for 'luke.cli' (ModuleNotFoundError: No module named 'luke')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##**Requirements**"
      ],
      "metadata": {
        "id": "_bbJIKTNYpiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install requirements"
      ],
      "metadata": {
        "id": "r7W6XdTl5LFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ice-luke/\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "kQJFJbpq5K4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install apex\n",
        "\n",
        "Note: this might take some time to install. If apex has already been downloaded it is sufficient to just copy the files to relevant locations."
      ],
      "metadata": {
        "id": "IpnNU_U25dGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = './apex/apex/'\n",
        "\n",
        "if not os.path.isdir(path):\n",
        "  !git clone https://github.com/NVIDIA/apex.git\n",
        "  !cd apex\n",
        "  !git checkout c3fad1ad120b23055f6630da0b029c8b626db78f\n",
        "  !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\n",
        "\n",
        "!cp -r apex/apex/ /usr/local/lib/python3.7/dist-packages/\n",
        "!cp -r apex/apex/ /usr/local/lib/python3.7/site-packages/"
      ],
      "metadata": {
        "id": "OFymi6RM5ddK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check if cuda and apex are available"
      ],
      "metadata": {
        "id": "uRgkVtkr5iMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from apex import amp\n",
        "\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "F80aN_Pd5iss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **Pretraining LUKE**\n",
        "\n",
        "As seen in [LUKE: Deep Contextualized Entity Representations with\n",
        "Entity-aware Self-attention](https://arxiv.org/pdf/2010.01057.pdf) by Yamada et al.\n",
        "\n",
        "\n",
        "Experiments for Icelandic were conducted on a Tesla V100-SXM2-16GB GPU and pretraining took roughly 16 hrs."
      ],
      "metadata": {
        "id": "gkxk1FPZux1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Make sure that a folder exists for the model output"
      ],
      "metadata": {
        "id": "Xrt2X3U7vLW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = 'luke-icebert-base-768'\n",
        "\n",
        "if not os.path.isdir(path):\n",
        "  os.mkdir(path)\n",
        "  print('{} created!'.format(path))\n",
        "else:\n",
        "  print('{} exists!'.format(path))"
      ],
      "metadata": {
        "id": "2e-drgyMvKkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretraining options"
      ],
      "metadata": {
        "id": "UJdLVAxcwlQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Options:\n",
        "  --multilingual\n",
        "  --sampling-smoothing FLOAT\n",
        "  --parallel\n",
        "  --cpu\n",
        "  --bert-model-name TEXT\n",
        "  --entity-emb-size INTEGER\n",
        "  --batch-size INTEGER\n",
        "  --gradient-accumulation-steps INTEGER\n",
        "  --learning-rate FLOAT\n",
        "  --lr-schedule [warmup_constant|warmup_linear]\n",
        "  --warmup-steps INTEGER\n",
        "  --adam-b1 FLOAT\n",
        "  --adam-b2 FLOAT\n",
        "  --adam-eps FLOAT\n",
        "  --weight-decay FLOAT\n",
        "  --max-grad-norm FLOAT\n",
        "  --masked-lm-prob FLOAT\n",
        "  --masked-entity-prob FLOAT\n",
        "  --whole-word-masking / --subword-masking\n",
        "  --unmasked-word-prob FLOAT\n",
        "  --random-word-prob FLOAT\n",
        "  --unmasked-entity-prob FLOAT\n",
        "  --random-entity-prob FLOAT\n",
        "  --mask-words-in-entity-span\n",
        "  --fix-bert-weights\n",
        "  --grad-avg-on-cpu / --grad-avg-on-gpu\n",
        "  --num-epochs INTEGER\n",
        "  --global-step INTEGER\n",
        "  --fp16\n",
        "  --fp16-opt-level [O1|O2]\n",
        "  --fp16-master-weights / --fp16-no-master-weights\n",
        "  --fp16-min-loss-scale INTEGER\n",
        "  --fp16-max-loss-scale INTEGER\n",
        "  --local-rank, --local_rank INTEGER\n",
        "  --num-nodes INTEGER\n",
        "  --node-rank INTEGER\n",
        "  --master-addr TEXT\n",
        "  --master-port TEXT\n",
        "  --log-dir PATH\n",
        "  --model-file PATH\n",
        "  --optimizer-file PATH\n",
        "  --scheduler-file PATH\n",
        "  --amp-file PATH\n",
        "  --save-interval-sec INTEGER\n",
        "  --save-interval-steps INTEGER\n",
        "  --help                          Show this\n",
        "                                  message and\n",
        "                                  exit.\n",
        "```"
      ],
      "metadata": {
        "id": "Mb-x_KLlwpmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pretraining\n",
        "\n",
        "We use IceBERT-base as our base model."
      ],
      "metadata": {
        "id": "43zjp-imvTgu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O5xM2Sqlkiu"
      },
      "outputs": [],
      "source": [
        "!python -m luke.cli pretrain ./is-wikipedia-pretrain-dataset/ ./luke-icebert-base-768/ \\\n",
        "--bert-model-name mideind/IceBERT-igc \\\n",
        "--entity-emb-size 768 \\\n",
        "--batch-size 8 \\\n",
        "--gradient-accumulation-steps 1 \\\n",
        "--learning-rate 1e-5 \\\n",
        "--warmup-steps 100 \\\n",
        "--log-dir ./logs/luke-icebert-base-768/ \\\n",
        "--weight-decay=0.01 \\\n",
        "--adam-b1=0.9 \\\n",
        "--adam-b2=0.999 \\\n",
        "--adam-eps=1e-6 \\\n",
        "--lr-schedule='warmup_linear' \\\n",
        "--masked-entity-prob=0.15 \\\n",
        "--masked-lm-prob=0.15"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **Pretraining LUKE ED**\n",
        "\n",
        "As seen in [Global Entity Disambiguation with Pretrained Contextualized Embeddings of Words and Entities](https://arxiv.org/pdf/1909.00426.pdf) by Yamada et al.\n",
        "\n",
        "\n",
        "Experiments for Icelandic were conducted on a Tesla V100-SXM2-16GB GPU and pretraining took roughly 16 hrs."
      ],
      "metadata": {
        "id": "mYrA5YYZvwig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m examples.cli entity-disambiguation create-redirect-tsv iswiki-20220101 iswiki-20220101-redirect.tsv"
      ],
      "metadata": {
        "id": "zFD5SP48zlkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wikipedia2vec build-dump-db iswiki-20220101-pages-articles.xml.bz2 iswiki-20220101"
      ],
      "metadata": {
        "id": "EDnYhp6zzp-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Make sure that a folder exists for the model output"
      ],
      "metadata": {
        "id": "5VKXgF49Zcux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "path = 'ed-luke-icebert-base-768'\n",
        "\n",
        "if not os.path.isdir(path):\n",
        "  os.mkdir(path)\n",
        "  print('{} created!'.format(path))\n",
        "else:\n",
        "  print('{} exists!'.format(path))"
      ],
      "metadata": {
        "id": "5RC9mzCWZdHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretraining options"
      ],
      "metadata": {
        "id": "cQEF9zO2Y6TE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Options:\n",
        "  --multilingual\n",
        "  --sampling-smoothing FLOAT\n",
        "  --parallel\n",
        "  --cpu\n",
        "  --bert-model-name TEXT\n",
        "  --entity-emb-size INTEGER\n",
        "  --batch-size INTEGER\n",
        "  --gradient-accumulation-steps INTEGER\n",
        "  --learning-rate FLOAT\n",
        "  --lr-schedule [warmup_constant|warmup_linear]\n",
        "  --warmup-steps INTEGER\n",
        "  --adam-b1 FLOAT\n",
        "  --adam-b2 FLOAT\n",
        "  --adam-eps FLOAT\n",
        "  --weight-decay FLOAT\n",
        "  --max-grad-norm FLOAT\n",
        "  --masked-lm-prob FLOAT\n",
        "  --masked-entity-prob FLOAT\n",
        "  --whole-word-masking / --subword-masking\n",
        "  --unmasked-word-prob FLOAT\n",
        "  --random-word-prob FLOAT\n",
        "  --unmasked-entity-prob FLOAT\n",
        "  --random-entity-prob FLOAT\n",
        "  --mask-words-in-entity-span\n",
        "  --fix-bert-weights\n",
        "  --grad-avg-on-cpu / --grad-avg-on-gpu\n",
        "  --num-epochs INTEGER\n",
        "  --global-step INTEGER\n",
        "  --fp16\n",
        "  --fp16-opt-level [O1|O2]\n",
        "  --fp16-master-weights / --fp16-no-master-weights\n",
        "  --fp16-min-loss-scale INTEGER\n",
        "  --fp16-max-loss-scale INTEGER\n",
        "  --local-rank, --local_rank INTEGER\n",
        "  --num-nodes INTEGER\n",
        "  --node-rank INTEGER\n",
        "  --master-addr TEXT\n",
        "  --master-port TEXT\n",
        "  --log-dir PATH\n",
        "  --model-file PATH\n",
        "  --optimizer-file PATH\n",
        "  --scheduler-file PATH\n",
        "  --amp-file PATH\n",
        "  --save-interval-sec INTEGER\n",
        "  --save-interval-steps INTEGER\n",
        "  --help                          Show this\n",
        "                                  message and\n",
        "                                  exit.\n",
        "```\n"
      ],
      "metadata": {
        "id": "LpKfqUSTZSHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pretraining\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Pretraining LUKE for ED is two staged. In the first stage training, the pretrained BERT parameters are fixed by setting `fix_bert_weights` to `True`. During this stage the model is trained using a learning rate of 5e-4 for one epoch. Then, for the second stahe, the pretraining continues by updating all the parameters with a learning rate of 5e-5 for six epochs. When the second stage pretraining starts the trained model parameters from the first stage are loaded by setting the `--model-file` to the model_file from stage one.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Note that `--masked-lm-prob` must be set to 0 since  masked language model is not used in the training. In addition, `--masked_entity_prob` is set to 0.3 for the experiments."
      ],
      "metadata": {
        "id": "Cwa4d1t-Y-6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####First Stage"
      ],
      "metadata": {
        "id": "jIZjRLiBjbHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m luke.cli pretrain ./is-wikipedia-pretrain-dataset/ ./ed-luke-icebert-base-768-first-stage/ \\\n",
        "--bert-model-name mideind/IceBERT-igc \\\n",
        "--entity-emb-size 768 \\\n",
        "--batch-size 8 \\\n",
        "--gradient-accumulation-steps 1 \\\n",
        "--learning-rate 5e-4 \\\n",
        "--warmup-steps 100 \\\n",
        "--log-dir ./logs/ed-luke-icebert-base-768-first-stage/ \\\n",
        "--weight-decay=0.01 \\\n",
        "--adam-b1=0.9 \\\n",
        "--adam-b2=0.999 \\\n",
        "--adam-eps=1e-6 \\\n",
        "--lr-schedule='warmup_linear' \\\n",
        "--masked-entity-prob=0.30 \\\n",
        "--masked-lm-prob=0 \\\n",
        "--max-grad-norm=1.0 \\\n",
        "--num-epochs 1 \\\n",
        "--fix-bert-weights"
      ],
      "metadata": {
        "id": "tfD_Cjj0wiJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Second Stage"
      ],
      "metadata": {
        "id": "s-CnoTNvjmRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m luke.cli pretrain ./is-wikipedia-pretrain-dataset/ ./ed-luke-icebert-base-768-second-stage/ \\\n",
        "--bert-model-name mideind/IceBERT-igc \\\n",
        "--model-file ./ed-luke-icebert-base-768-first-stage/model_epoch1.bin \\\n",
        "--entity-emb-size 768 \\\n",
        "--batch-size 8 \\\n",
        "--gradient-accumulation-steps 1 \\\n",
        "--learning-rate 5e-5 \\\n",
        "--warmup-steps 100 \\\n",
        "--log-dir ./logs/ed-luke-icebert-base-768-second-stage/ \\\n",
        "--weight-decay=0.01 \\\n",
        "--adam-b1=0.9 \\\n",
        "--adam-b2=0.999 \\\n",
        "--adam-eps=1e-6 \\\n",
        "--lr-schedule='warmup_linear' \\\n",
        "--masked-entity-prob=0.30 \\\n",
        "--masked-lm-prob=0 \\\n",
        "--max-grad-norm=1.0 \\\n",
        "--num-epochs 6"
      ],
      "metadata": {
        "id": "3i5zwY87jkZq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}